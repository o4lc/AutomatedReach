{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jun  1 17:45:41 2020\n",
    "\n",
    "@author: mahyarfazlyab\n",
    "\"\"\"\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"../Python/\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.io import loadmat\n",
    "import scipy.io\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def export2matlab(file_name, net, A, B, save_model=False):\n",
    "    '''\n",
    "    Export pytorch fully connected network to matlab\n",
    "\n",
    "    '''\n",
    "\n",
    "    num_layers = int((len(net) - 1) / 2)\n",
    "    dim_in = float(net[0].weight.shape[1])\n",
    "    dim_out = float(net[-1].weight.shape[0])\n",
    "    hidden_dims = [float(net[2 * i].weight.shape[0]) for i in range(0, num_layers)]\n",
    "\n",
    "    # network dimensions\n",
    "    dims = [dim_in] + hidden_dims + [dim_out]\n",
    "\n",
    "    # get weights\n",
    "    # weights = np.zeros((num_layers+1,))\n",
    "    weights = [net[2 * i].weight.detach().numpy().astype(np.float64) for i in range(0, num_layers + 1)]\n",
    "\n",
    "    # get biases\n",
    "    # biases = np.zeros((num_layers+1,))\n",
    "    biases = [net[2 * i].bias.detach().numpy().astype(np.float64).reshape(-1, 1) for i in range(0, num_layers + 1)]\n",
    "\n",
    "    activation = str(net[1])[0:-2].lower()\n",
    "\n",
    "    # export network data to matlab\n",
    "    data = {}\n",
    "    data['net'] = {'weights': weights, 'biases': biases, 'dims': dims, 'activation': activation, 'name': file_name}\n",
    "    data['AMatrix'] = A\n",
    "    data['BMatrix'] = B\n",
    "\n",
    "    scipy.io.savemat(file_name + '.mat', data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    with open('xs.pkl', 'rb') as f:\n",
    "        Xtrain = pickle.load(f)\n",
    "\n",
    "    with open('us.pkl', 'rb') as f:\n",
    "        Ytrain = pickle.load(f)\n",
    "\n",
    "    # data = loadmat(\"quadRotorTrainData.mat\")\n",
    "    # Xtrain = data['Xtrain']\n",
    "    # Ytrain = data['Ytrain']\n",
    "    # A = data['A']\n",
    "    # B = data['B']\n",
    "\n",
    "    A = np.zeros((6, 6))\n",
    "    A[0, 3] = 1\n",
    "    A[1, 4] = 1\n",
    "    A[2, 5] = 1\n",
    "\n",
    "    B = np.zeros((6, 3))\n",
    "    B[3, 0] =  9.8\n",
    "    B[4, 1] = -9.8\n",
    "    B[5, 2] =  1\n",
    "\n",
    "    A = np.eye(6) + A * 0.1\n",
    "    B = B * 0.1\n",
    "    print(A)\n",
    "    print(B)\n",
    "    print(Xtrain.shape)\n",
    "    print(Ytrain.shape)\n",
    "    trainset = torch.utils.data.TensorDataset(torch.Tensor(Xtrain), torch.Tensor(Ytrain))\n",
    "\n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(6, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 3))\n",
    "\n",
    "    train_batch_size = 128\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=train_batch_size, shuffle=True,\n",
    "                                              num_workers=2)\n",
    "    epoch = 2000\n",
    "    net.train()\n",
    "    losses = []\n",
    "    criterion = nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
    "    optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "    for t in range(epoch):\n",
    "        for i, (X, Y) in enumerate(trainloader):\n",
    "            out = net(X)\n",
    "            loss = criterion(out, Y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        if (np.mod(t, 10) == 0):\n",
    "            print('epoch: ', t, 'MSE loss: ', loss.item())\n",
    "\n",
    "    export2matlab('quadRotor' , net, A, B, save_model=True)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0.  0.  0.1 0.  0. ]\n",
      " [0.  1.  0.  0.  0.1 0. ]\n",
      " [0.  0.  1.  0.  0.  0.1]\n",
      " [0.  0.  0.  1.  0.  0. ]\n",
      " [0.  0.  0.  0.  1.  0. ]\n",
      " [0.  0.  0.  0.  0.  1. ]]\n",
      "[[ 0.    0.    0.  ]\n",
      " [ 0.    0.    0.  ]\n",
      " [ 0.    0.    0.  ]\n",
      " [ 0.98  0.    0.  ]\n",
      " [ 0.   -0.98  0.  ]\n",
      " [ 0.    0.    0.1 ]]\n",
      "(4961, 6)\n",
      "(4961, 3)\n",
      "epoch:  0 MSE loss:  0.062010474503040314\n",
      "epoch:  10 MSE loss:  0.011202693916857243\n",
      "epoch:  20 MSE loss:  0.00807721633464098\n",
      "epoch:  30 MSE loss:  0.005317296367138624\n",
      "epoch:  40 MSE loss:  0.003956475295126438\n",
      "epoch:  50 MSE loss:  0.004558625165373087\n",
      "epoch:  60 MSE loss:  0.0037152012810111046\n",
      "epoch:  70 MSE loss:  0.0034782919101417065\n",
      "epoch:  80 MSE loss:  0.0024149331729859114\n",
      "epoch:  90 MSE loss:  0.0022053178399801254\n",
      "epoch:  100 MSE loss:  0.0026356433518230915\n",
      "epoch:  110 MSE loss:  0.0018915917025879025\n",
      "epoch:  120 MSE loss:  0.00231808889657259\n",
      "epoch:  130 MSE loss:  0.00302883330732584\n",
      "epoch:  140 MSE loss:  0.002061219187453389\n",
      "epoch:  150 MSE loss:  0.002625806024298072\n",
      "epoch:  160 MSE loss:  0.0013374420814216137\n",
      "epoch:  170 MSE loss:  0.001726504066027701\n",
      "epoch:  180 MSE loss:  0.001995462691411376\n",
      "epoch:  190 MSE loss:  0.0016576512716710567\n",
      "epoch:  200 MSE loss:  0.001719287014566362\n",
      "epoch:  210 MSE loss:  0.001478087273426354\n",
      "epoch:  220 MSE loss:  0.001913209562189877\n",
      "epoch:  230 MSE loss:  0.0014241740573197603\n",
      "epoch:  240 MSE loss:  0.0014805227983742952\n",
      "epoch:  250 MSE loss:  0.002158262999728322\n",
      "epoch:  260 MSE loss:  0.0010915768798440695\n",
      "epoch:  270 MSE loss:  0.001996129984036088\n",
      "epoch:  280 MSE loss:  0.0014683548361063004\n",
      "epoch:  290 MSE loss:  0.0012561326147988439\n",
      "epoch:  300 MSE loss:  0.0015567033551633358\n",
      "epoch:  310 MSE loss:  0.0015696824993938208\n",
      "epoch:  320 MSE loss:  0.0012302768882364035\n",
      "epoch:  330 MSE loss:  0.0016139543149620295\n",
      "epoch:  340 MSE loss:  0.001397129031829536\n",
      "epoch:  350 MSE loss:  0.001702987472526729\n",
      "epoch:  360 MSE loss:  0.0012347308220341802\n",
      "epoch:  370 MSE loss:  0.0013236431404948235\n",
      "epoch:  380 MSE loss:  0.001648159115575254\n",
      "epoch:  390 MSE loss:  0.001403592643328011\n",
      "epoch:  400 MSE loss:  0.0016520451754331589\n",
      "epoch:  410 MSE loss:  0.0019438453018665314\n",
      "epoch:  420 MSE loss:  0.0010232964996248484\n",
      "epoch:  430 MSE loss:  0.001430631848052144\n",
      "epoch:  440 MSE loss:  0.0016478239558637142\n",
      "epoch:  450 MSE loss:  0.001287787570618093\n",
      "epoch:  460 MSE loss:  0.0021115036215633154\n",
      "epoch:  470 MSE loss:  0.0013148317812010646\n",
      "epoch:  480 MSE loss:  0.0017550071934238076\n",
      "epoch:  490 MSE loss:  0.0013754460960626602\n",
      "epoch:  500 MSE loss:  0.0012744320556521416\n",
      "epoch:  510 MSE loss:  0.0012195174349471927\n",
      "epoch:  520 MSE loss:  0.0022058470640331507\n",
      "epoch:  530 MSE loss:  0.0017282364424318075\n",
      "epoch:  540 MSE loss:  0.001752530806697905\n",
      "epoch:  550 MSE loss:  0.001675437088124454\n",
      "epoch:  560 MSE loss:  0.0014797617914155126\n",
      "epoch:  570 MSE loss:  0.0019196722423657775\n",
      "epoch:  580 MSE loss:  0.0013996693305671215\n",
      "epoch:  590 MSE loss:  0.0010589442681521177\n",
      "epoch:  600 MSE loss:  0.0012356191873550415\n",
      "epoch:  610 MSE loss:  0.0012730611488223076\n",
      "epoch:  620 MSE loss:  0.001250363769941032\n",
      "epoch:  630 MSE loss:  0.002083706669509411\n",
      "epoch:  640 MSE loss:  0.0020621917210519314\n",
      "epoch:  650 MSE loss:  0.0013949107378721237\n",
      "epoch:  660 MSE loss:  0.0019464250653982162\n",
      "epoch:  670 MSE loss:  0.0010365855414420366\n",
      "epoch:  680 MSE loss:  0.001345090800896287\n",
      "epoch:  690 MSE loss:  0.0010380166349932551\n",
      "epoch:  700 MSE loss:  0.0011260205646976829\n",
      "epoch:  710 MSE loss:  0.0014210449298843741\n",
      "epoch:  720 MSE loss:  0.0008402011590078473\n",
      "epoch:  730 MSE loss:  0.0011045646388083696\n",
      "epoch:  740 MSE loss:  0.0015154884895309806\n",
      "epoch:  750 MSE loss:  0.0015726767014712095\n",
      "epoch:  760 MSE loss:  0.0012037173146381974\n",
      "epoch:  770 MSE loss:  0.001020745257847011\n",
      "epoch:  780 MSE loss:  0.0009083058685064316\n",
      "epoch:  790 MSE loss:  0.0010739825665950775\n",
      "epoch:  800 MSE loss:  0.0016851171385496855\n",
      "epoch:  810 MSE loss:  0.001705748145468533\n",
      "epoch:  820 MSE loss:  0.0011375703616067767\n",
      "epoch:  830 MSE loss:  0.0011914708884432912\n",
      "epoch:  840 MSE loss:  0.0017677537398412824\n",
      "epoch:  850 MSE loss:  0.0013052144786342978\n",
      "epoch:  860 MSE loss:  0.0009729497833177447\n",
      "epoch:  870 MSE loss:  0.0010416456498205662\n",
      "epoch:  880 MSE loss:  0.0010340121807530522\n",
      "epoch:  890 MSE loss:  0.001431695418432355\n",
      "epoch:  900 MSE loss:  0.0008397053461521864\n",
      "epoch:  910 MSE loss:  0.0008503122953698039\n",
      "epoch:  920 MSE loss:  0.0009064832120202482\n",
      "epoch:  930 MSE loss:  0.0010659394320100546\n",
      "epoch:  940 MSE loss:  0.0010469170520082116\n",
      "epoch:  950 MSE loss:  0.0011265100911259651\n",
      "epoch:  960 MSE loss:  0.0017945008585229516\n",
      "epoch:  970 MSE loss:  0.001526676001958549\n",
      "epoch:  980 MSE loss:  0.0018666456453502178\n",
      "epoch:  990 MSE loss:  0.0008143367595039308\n",
      "epoch:  1000 MSE loss:  0.0005985233001410961\n",
      "epoch:  1010 MSE loss:  0.0010416315635666251\n",
      "epoch:  1020 MSE loss:  0.000912788207642734\n",
      "epoch:  1030 MSE loss:  0.0009818478720262647\n",
      "epoch:  1040 MSE loss:  0.0007852833368815482\n",
      "epoch:  1050 MSE loss:  0.0013838758459314704\n",
      "epoch:  1060 MSE loss:  0.000998605857603252\n",
      "epoch:  1070 MSE loss:  0.0012389931362122297\n",
      "epoch:  1080 MSE loss:  0.0010627659503370523\n",
      "epoch:  1090 MSE loss:  0.0012751794420182705\n",
      "epoch:  1100 MSE loss:  0.001485744258388877\n",
      "epoch:  1110 MSE loss:  0.0008432078175246716\n",
      "epoch:  1120 MSE loss:  0.0012682805536314845\n",
      "epoch:  1130 MSE loss:  0.001288647297769785\n",
      "epoch:  1140 MSE loss:  0.001406922354362905\n",
      "epoch:  1150 MSE loss:  0.0013321955921128392\n",
      "epoch:  1160 MSE loss:  0.0012348397867754102\n",
      "epoch:  1170 MSE loss:  0.0010105908149853349\n",
      "epoch:  1180 MSE loss:  0.000942644604947418\n",
      "epoch:  1190 MSE loss:  0.0012950330274179578\n",
      "epoch:  1200 MSE loss:  0.0008613503887318075\n",
      "epoch:  1210 MSE loss:  0.0011929322499781847\n",
      "epoch:  1220 MSE loss:  0.0010686820605769753\n",
      "epoch:  1230 MSE loss:  0.0013021485647186637\n",
      "epoch:  1240 MSE loss:  0.0013047379907220602\n",
      "epoch:  1250 MSE loss:  0.0014636761043220758\n",
      "epoch:  1260 MSE loss:  0.0010836265282705426\n",
      "epoch:  1270 MSE loss:  0.0013973484747111797\n",
      "epoch:  1280 MSE loss:  0.001148501643911004\n",
      "epoch:  1290 MSE loss:  0.0010565962875261903\n",
      "epoch:  1300 MSE loss:  0.0006095010903663933\n",
      "epoch:  1310 MSE loss:  0.0005230271490290761\n",
      "epoch:  1320 MSE loss:  0.0008365424582734704\n",
      "epoch:  1330 MSE loss:  0.0012216591276228428\n",
      "epoch:  1340 MSE loss:  0.0011533547658473253\n",
      "epoch:  1350 MSE loss:  0.001197025179862976\n",
      "epoch:  1360 MSE loss:  0.000758822716306895\n",
      "epoch:  1370 MSE loss:  0.0012088887160643935\n",
      "epoch:  1380 MSE loss:  0.0011346446117386222\n",
      "epoch:  1390 MSE loss:  0.0009846709435805678\n",
      "epoch:  1400 MSE loss:  0.0009822988649830222\n",
      "epoch:  1410 MSE loss:  0.0009644333622418344\n",
      "epoch:  1420 MSE loss:  0.001078761531971395\n",
      "epoch:  1430 MSE loss:  0.001372149563394487\n",
      "epoch:  1440 MSE loss:  0.001326875644735992\n",
      "epoch:  1450 MSE loss:  0.0013937222538515925\n",
      "epoch:  1460 MSE loss:  0.0009389004553668201\n",
      "epoch:  1470 MSE loss:  0.001263893791474402\n",
      "epoch:  1480 MSE loss:  0.000817872816696763\n",
      "epoch:  1490 MSE loss:  0.0008684670319780707\n",
      "epoch:  1500 MSE loss:  0.0011507229646667838\n",
      "epoch:  1510 MSE loss:  0.000902265717741102\n",
      "epoch:  1520 MSE loss:  0.0012933127582073212\n",
      "epoch:  1530 MSE loss:  0.0011012082686647773\n",
      "epoch:  1540 MSE loss:  0.0009788016323000193\n",
      "epoch:  1550 MSE loss:  0.0008858180372044444\n",
      "epoch:  1560 MSE loss:  0.001126692513935268\n",
      "epoch:  1570 MSE loss:  0.0007555662887170911\n",
      "epoch:  1580 MSE loss:  0.001052434672601521\n",
      "epoch:  1590 MSE loss:  0.0008378805941902101\n",
      "epoch:  1600 MSE loss:  0.0008369173156097531\n",
      "epoch:  1610 MSE loss:  0.000847008777782321\n",
      "epoch:  1620 MSE loss:  0.0012631850549951196\n",
      "epoch:  1630 MSE loss:  0.0011912088375538588\n",
      "epoch:  1640 MSE loss:  0.0008414383046329021\n",
      "epoch:  1650 MSE loss:  0.00150717340875417\n",
      "epoch:  1660 MSE loss:  0.0008837917703203857\n",
      "epoch:  1670 MSE loss:  0.0007902397192083299\n",
      "epoch:  1680 MSE loss:  0.0006477541173808277\n",
      "epoch:  1690 MSE loss:  0.0010676664533093572\n",
      "epoch:  1700 MSE loss:  0.000685649982187897\n",
      "epoch:  1710 MSE loss:  0.0013554083416238427\n",
      "epoch:  1720 MSE loss:  0.0013294838136062026\n",
      "epoch:  1730 MSE loss:  0.001148179522715509\n",
      "epoch:  1740 MSE loss:  0.0009955011773854494\n",
      "epoch:  1750 MSE loss:  0.0018089179648086429\n",
      "epoch:  1760 MSE loss:  0.0009752535261213779\n",
      "epoch:  1770 MSE loss:  0.0014958136016502976\n",
      "epoch:  1780 MSE loss:  0.0013851169496774673\n",
      "epoch:  1790 MSE loss:  0.001095358282327652\n",
      "epoch:  1800 MSE loss:  0.0012347452575340867\n",
      "epoch:  1810 MSE loss:  0.0007634244975633919\n",
      "epoch:  1820 MSE loss:  0.0006714817718602717\n",
      "epoch:  1830 MSE loss:  0.0008692296105436981\n",
      "epoch:  1840 MSE loss:  0.0015938413562253118\n",
      "epoch:  1850 MSE loss:  0.0009824775625020266\n",
      "epoch:  1860 MSE loss:  0.001108826487325132\n",
      "epoch:  1870 MSE loss:  0.001050183200277388\n",
      "epoch:  1880 MSE loss:  0.001090837293304503\n",
      "epoch:  1890 MSE loss:  0.0009679906070232391\n",
      "epoch:  1900 MSE loss:  0.0009677473572082818\n",
      "epoch:  1910 MSE loss:  0.001520701334811747\n",
      "epoch:  1920 MSE loss:  0.00140607007779181\n",
      "epoch:  1930 MSE loss:  0.0009727611322887242\n",
      "epoch:  1940 MSE loss:  0.0006942531908862293\n",
      "epoch:  1950 MSE loss:  0.0012326454743742943\n",
      "epoch:  1960 MSE loss:  0.0005334576708264649\n",
      "epoch:  1970 MSE loss:  0.001176587538793683\n",
      "epoch:  1980 MSE loss:  0.0017714707646518946\n",
      "epoch:  1990 MSE loss:  0.001402876223437488\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    losses = main()\n",
    "    plt.plot(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sina/Desktop/nn-verification-bnb-lip/Networks/6DQuadrotos dataset/temp.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sina/Desktop/nn-verification-bnb-lip/Networks/6DQuadrotos%20dataset/temp.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sina/Desktop/nn-verification-bnb-lip/Networks/6DQuadrotos%20dataset/temp.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(losses)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
