{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`polytope` failed to import `cvxopt.glpk`.\n",
      "will use `scipy.optimize.linprog`\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jun  1 17:45:41 2020\n",
    "\n",
    "@author: mahyarfazlyab\n",
    "\"\"\"\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(\"../Python/\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.io import loadmat\n",
    "import scipy.io\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from NeuralNetwork import NeuralNetwork\n",
    "\n",
    "\n",
    "\n",
    "def export2matlab(file_name, net, A, B, save_model=False):\n",
    "    '''\n",
    "    Export pytorch fully connected network to matlab\n",
    "\n",
    "    '''\n",
    "\n",
    "    num_layers = int((len(net) - 1) / 2)\n",
    "    dim_in = float(net[0].weight.shape[1])\n",
    "    dim_out = float(net[-1].weight.shape[0])\n",
    "    hidden_dims = [float(net[2 * i].weight.shape[0]) for i in range(0, num_layers)]\n",
    "\n",
    "    # network dimensions\n",
    "    dims = [dim_in] + hidden_dims + [dim_out]\n",
    "\n",
    "    # get weights\n",
    "    # weights = np.zeros((num_layers+1,))\n",
    "    weights = [net[2 * i].weight.detach().numpy().astype(np.float64) for i in range(0, num_layers + 1)]\n",
    "\n",
    "    # get biases\n",
    "    # biases = np.zeros((num_layers+1,))\n",
    "    biases = [net[2 * i].bias.detach().numpy().astype(np.float64).reshape(-1, 1) for i in range(0, num_layers + 1)]\n",
    "\n",
    "    activation = str(net[1])[0:-2].lower()\n",
    "\n",
    "    # export network data to matlab\n",
    "    data = {}\n",
    "    data['net'] = {'weights': weights, 'biases': biases, 'dims': dims, 'activation': activation, 'name': file_name}\n",
    "    data['AMatrix'] = A\n",
    "    data['BMatrix'] = B\n",
    "\n",
    "    scipy.io.savemat(file_name + '.mat', data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "trainCompleteLoop = True\n",
    "# with open('xs.pkl', 'rb') as f:\n",
    "#     Xtrain = pickle.load(f)\n",
    "#\n",
    "# with open('us.pkl', 'rb') as f:\n",
    "#     Ytrain = pickle.load(f)\n",
    "\n",
    "data = loadmat(\"trainDataQuad.mat\")\n",
    "Xtrain = data['X']\n",
    "Ytrain = data['y']\n",
    "# data = loadmat(\"quadRotorTrainData.mat\")\n",
    "# Xtrain = data['Xtrain']\n",
    "# Ytrain = data['Ytrain']\n",
    "# A = data['A']\n",
    "# B = data['B']\n",
    "if True:\n",
    "    A = np.zeros((6, 6))\n",
    "    A[0, 3] = 1\n",
    "    A[1, 4] = 1\n",
    "    A[2, 5] = 1\n",
    "\n",
    "    B = np.zeros((6, 3))\n",
    "    B[3, 0] =  9.8\n",
    "    B[4, 1] = -9.8\n",
    "    B[5, 2] =  1\n",
    "\n",
    "    C = np.zeros((6, 1))\n",
    "    C[5] = -9.8\n",
    "\n",
    "    A = np.eye(6) + A * 0.1\n",
    "    B = B * 0.1\n",
    "    C = C * 0.1\n",
    "\n",
    "else:\n",
    "    A = np.array([[1, 1], [0, 1]])\n",
    "    B = np.array([[0.5], [1]])\n",
    "lastDim = 3\n",
    "if trainCompleteLoop:\n",
    "    Ytrain = (A @ Xtrain.T + B @ Ytrain.T + C).T\n",
    "    lastDim = 6\n",
    "\n",
    "# print(torch.linalg.norm(torch.Tensor(B)), torch.linalg.norm(torch.Tensor(A)))\n",
    "# raise\n",
    "# print(A)\n",
    "# print(B)\n",
    "# print(Xtrain.shape)\n",
    "# print(Ytrain.shape)\n",
    "trainset = torch.utils.data.TensorDataset(torch.Tensor(Xtrain), torch.Tensor(Ytrain))\n",
    "\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(6, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, lastDim))\n",
    "\n",
    "train_batch_size = 128\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=train_batch_size, shuffle=True,\n",
    "                                          num_workers=2)\n",
    "epoch = 2000\n",
    "net.train()\n",
    "losses = []\n",
    "\n",
    "def my_loss(output, target):\n",
    "    gamma = 0.2\n",
    "    loss = torch.mean((output - target)**2) + gamma * torch.linalg.norm(net[-1].weight) \\\n",
    "                                                * torch.linalg.norm(net[-3].weight) \\\n",
    "                                                * torch.linalg.norm(net[-5].weight)\n",
    "    return loss\n",
    "bestStateDic = None\n",
    "bestLoss = 100000\n",
    "criterion = my_loss\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "for t in range(epoch):\n",
    "    for i, (X, Y) in enumerate(trainloader):\n",
    "        out = net(X)\n",
    "        loss = criterion(out, Y)\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            optimizer.zero_grad()\n",
    "            for i in [0, 2, 4]:\n",
    "                net[i].weight /= torch.maximum(torch.tensor([1]), (torch.linalg.norm(net[i].weight) / 1.))\n",
    "\n",
    "            if loss < bestLoss:\n",
    "                bestLoss = loss\n",
    "                bestStateDic = net.state_dict()\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    if (np.mod(t, 10) == 0):\n",
    "        print('epoch: ', t, 'MSE loss: ', loss.item())\n",
    "        print(torch.linalg.norm(net[0].weight) * torch.linalg.norm(net[2].weight) * torch.linalg.norm(net[4].weight))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "layers = []\n",
    "dimensions = [6, 32, 32, lastDim]\n",
    "for i in range(len(dimensions) - 1):\n",
    "    layers.append(nn.Linear(dimensions[i], dimensions[i + 1]))\n",
    "    if i < len(dimensions) - 2:\n",
    "        layers.append(nn.ReLU())\n",
    "network = nn.Sequential(*layers)\n",
    "networkClass = NeuralNetwork(\"../randomNetwork.pth\")\n",
    "\n",
    "networkClass.Linear = network\n",
    "torch.save(networkClass.state_dict(), \"../quadRotorCompleteLoopVq.0.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 MSE loss:  3.611684560775757\n",
      "tensor(0.9194, grad_fn=<MulBackward0>)\n",
      "epoch:  10 MSE loss:  3.0650851726531982\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  20 MSE loss:  2.995669364929199\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  30 MSE loss:  3.417895793914795\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  40 MSE loss:  3.105945110321045\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  50 MSE loss:  3.208569049835205\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  60 MSE loss:  2.981788396835327\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  70 MSE loss:  2.756409168243408\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  80 MSE loss:  2.5782480239868164\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  90 MSE loss:  3.0307068824768066\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  100 MSE loss:  3.0084340572357178\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  110 MSE loss:  3.0354719161987305\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  120 MSE loss:  3.121391773223877\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  130 MSE loss:  2.9903171062469482\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  140 MSE loss:  2.6648504734039307\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  150 MSE loss:  2.9635114669799805\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  160 MSE loss:  2.6323533058166504\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  170 MSE loss:  2.598170042037964\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  180 MSE loss:  2.8791489601135254\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  190 MSE loss:  2.8169026374816895\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  200 MSE loss:  2.689767360687256\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  210 MSE loss:  3.0257835388183594\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  220 MSE loss:  2.628394842147827\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  230 MSE loss:  2.880561351776123\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  240 MSE loss:  2.7530524730682373\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  250 MSE loss:  2.9416990280151367\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  260 MSE loss:  3.1658692359924316\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  270 MSE loss:  2.8001880645751953\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  280 MSE loss:  2.7881484031677246\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  290 MSE loss:  3.0383410453796387\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  300 MSE loss:  2.99544620513916\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  310 MSE loss:  2.834395170211792\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  320 MSE loss:  3.0514392852783203\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  330 MSE loss:  2.4712913036346436\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  340 MSE loss:  2.8013153076171875\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  350 MSE loss:  2.840700149536133\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  360 MSE loss:  2.843198537826538\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  370 MSE loss:  2.8086893558502197\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  380 MSE loss:  3.0585525035858154\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  390 MSE loss:  3.2537753582000732\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  400 MSE loss:  2.777608871459961\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  410 MSE loss:  2.9400644302368164\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  420 MSE loss:  2.805182456970215\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  430 MSE loss:  3.062492847442627\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  440 MSE loss:  2.9802703857421875\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  450 MSE loss:  2.778144121170044\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  460 MSE loss:  2.922764539718628\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  470 MSE loss:  3.153186321258545\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  480 MSE loss:  2.536482572555542\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  490 MSE loss:  2.630516767501831\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  500 MSE loss:  2.7698655128479004\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  510 MSE loss:  2.855787515640259\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  520 MSE loss:  2.692713499069214\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  530 MSE loss:  2.4093847274780273\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  540 MSE loss:  2.933600664138794\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  550 MSE loss:  3.08079195022583\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  560 MSE loss:  2.9530889987945557\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  570 MSE loss:  2.917538642883301\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  580 MSE loss:  2.6417882442474365\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  590 MSE loss:  2.452164888381958\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  600 MSE loss:  2.7109687328338623\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  610 MSE loss:  2.683529853820801\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  620 MSE loss:  2.631436586380005\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  630 MSE loss:  2.6913928985595703\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  640 MSE loss:  2.737248182296753\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  650 MSE loss:  2.8935351371765137\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  660 MSE loss:  2.9828696250915527\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  670 MSE loss:  2.892770767211914\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  680 MSE loss:  2.8988113403320312\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  690 MSE loss:  2.7402539253234863\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  700 MSE loss:  2.715252637863159\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  710 MSE loss:  2.8175134658813477\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  720 MSE loss:  2.7769575119018555\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  730 MSE loss:  2.882270097732544\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  740 MSE loss:  2.946730852127075\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  750 MSE loss:  2.505056381225586\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  760 MSE loss:  3.01576566696167\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  770 MSE loss:  2.975778579711914\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  780 MSE loss:  2.7416348457336426\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  790 MSE loss:  2.873781442642212\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  800 MSE loss:  2.9285528659820557\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  810 MSE loss:  2.975745916366577\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  820 MSE loss:  2.834261178970337\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  830 MSE loss:  2.8245327472686768\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  840 MSE loss:  2.782876968383789\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  850 MSE loss:  2.9689159393310547\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  860 MSE loss:  3.039362907409668\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  870 MSE loss:  2.6642227172851562\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  880 MSE loss:  3.1379036903381348\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  890 MSE loss:  2.787937641143799\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  900 MSE loss:  3.00455641746521\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  910 MSE loss:  2.619398832321167\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  920 MSE loss:  2.728750705718994\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  930 MSE loss:  2.4952735900878906\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  940 MSE loss:  3.2895233631134033\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  950 MSE loss:  2.8174381256103516\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  960 MSE loss:  2.868292808532715\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  970 MSE loss:  3.1105144023895264\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  980 MSE loss:  2.885303497314453\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  990 MSE loss:  2.9121642112731934\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1000 MSE loss:  2.4739179611206055\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1010 MSE loss:  3.010153293609619\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1020 MSE loss:  2.694850444793701\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1030 MSE loss:  2.926265239715576\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1040 MSE loss:  2.925382137298584\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1050 MSE loss:  2.5843610763549805\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1060 MSE loss:  3.2058403491973877\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1070 MSE loss:  2.815964460372925\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1080 MSE loss:  2.7354214191436768\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1090 MSE loss:  2.9714014530181885\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1100 MSE loss:  3.131068468093872\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1110 MSE loss:  2.6998541355133057\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1120 MSE loss:  3.069514513015747\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1130 MSE loss:  2.9207358360290527\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1140 MSE loss:  3.0584352016448975\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1150 MSE loss:  2.833829402923584\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1160 MSE loss:  2.8223323822021484\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1170 MSE loss:  3.019318103790283\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1180 MSE loss:  2.7453408241271973\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1190 MSE loss:  2.766118288040161\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1200 MSE loss:  2.9844613075256348\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  1210 MSE loss:  2.9647111892700195\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1220 MSE loss:  2.6467092037200928\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  1230 MSE loss:  2.976879596710205\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1240 MSE loss:  2.9184937477111816\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1250 MSE loss:  2.5927340984344482\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1260 MSE loss:  3.0500118732452393\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1270 MSE loss:  2.8521485328674316\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1280 MSE loss:  2.6595940589904785\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  1290 MSE loss:  2.590790033340454\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1300 MSE loss:  3.1995325088500977\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1310 MSE loss:  2.657975673675537\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1320 MSE loss:  2.906014919281006\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1330 MSE loss:  2.959604501724243\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1340 MSE loss:  2.7507920265197754\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1350 MSE loss:  2.838035821914673\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1360 MSE loss:  2.7607922554016113\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1370 MSE loss:  2.809582233428955\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1380 MSE loss:  3.017594814300537\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1390 MSE loss:  2.9102671146392822\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  1400 MSE loss:  3.2254178524017334\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  1410 MSE loss:  2.9489951133728027\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1420 MSE loss:  2.9859330654144287\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1430 MSE loss:  2.600813627243042\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  1440 MSE loss:  2.860408067703247\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  1450 MSE loss:  3.0549156665802\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  1460 MSE loss:  2.937215805053711\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1470 MSE loss:  2.688335657119751\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1480 MSE loss:  2.9480419158935547\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1490 MSE loss:  2.9353976249694824\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1500 MSE loss:  2.826993227005005\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1510 MSE loss:  2.913115978240967\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1520 MSE loss:  3.0176382064819336\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1530 MSE loss:  3.0057759284973145\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1540 MSE loss:  2.893125295639038\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1550 MSE loss:  2.909834146499634\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1560 MSE loss:  2.7705283164978027\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1570 MSE loss:  2.8084192276000977\n",
      "tensor(1., grad_fn=<MulBackward0>)\n",
      "epoch:  1580 MSE loss:  2.958369016647339\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1590 MSE loss:  2.829707622528076\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1600 MSE loss:  2.8175086975097656\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1610 MSE loss:  2.8192012310028076\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1620 MSE loss:  2.950941562652588\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n",
      "epoch:  1630 MSE loss:  2.2604966163635254\n",
      "tensor(1.0000, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/__init__.py\", line 16, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "  File \"/opt/homebrew/lib/python3.9/site-packages/torch/__init__.py\", line 14, in <module>\n",
      "    import platform\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/platform.py\", line 156, in <module>\n",
      "    from . import context\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/context.py\", line 6, in <module>\n",
      "    _libc_search = re.compile(b'(__libc_init)'\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/re.py\", line 252, in compile\n",
      "    from . import reduction\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 846, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 941, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1039, in get_data\n",
      "KeyboardInterrupt\n",
      "    return _compile(pattern, flags)\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/re.py\", line 304, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/sre_compile.py\", line 768, in compile\n",
      "    code = _code(p, flags)\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/sre_compile.py\", line 607, in _code\n",
      "    _compile(code, p.data, flags)\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/sre_compile.py\", line 209, in _compile\n",
      "    _compile(code, av, flags)\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/sre_compile.py\", line 168, in _compile\n",
      "    _compile(code, p, _combine_flags(flags, add_flags, del_flags))\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/sre_compile.py\", line 156, in _compile\n",
      "    _compile(code, av[2], flags)\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/sre_compile.py\", line 168, in _compile\n",
      "    _compile(code, p, _combine_flags(flags, add_flags, del_flags))\n",
      "  File \"/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/sre_compile.py\", line 149, in _compile\n",
      "    emit(SUCCESS)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [17]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m losses \u001B[38;5;241m=\u001B[39m main()\n\u001B[1;32m      2\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(losses)\n",
      "Input \u001B[0;32mIn [16]\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     75\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(net\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m)\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epoch):\n\u001B[0;32m---> 77\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, (X, Y) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(trainloader):\n\u001B[1;32m     78\u001B[0m         out \u001B[38;5;241m=\u001B[39m net(X)\n\u001B[1;32m     79\u001B[0m         loss \u001B[38;5;241m=\u001B[39m criterion(out, Y)\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/dataloader.py:530\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    528\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    529\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[0;32m--> 530\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    531\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    532\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    533\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    534\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1207\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1204\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[1;32m   1206\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1207\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1208\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1209\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[1;32m   1210\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1173\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1169\u001B[0m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[1;32m   1170\u001B[0m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[1;32m   1171\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1172\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m-> 1173\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1174\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[1;32m   1175\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1011\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    998\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_try_get_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m_utils\u001B[38;5;241m.\u001B[39mMP_STATUS_CHECK_INTERVAL):\n\u001B[1;32m    999\u001B[0m     \u001B[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001B[39;00m\n\u001B[1;32m   1000\u001B[0m     \u001B[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1008\u001B[0m     \u001B[38;5;66;03m# Returns a 2-tuple:\u001B[39;00m\n\u001B[1;32m   1009\u001B[0m     \u001B[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001B[39;00m\n\u001B[1;32m   1010\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1011\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1012\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n\u001B[1;32m   1013\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1014\u001B[0m         \u001B[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001B[39;00m\n\u001B[1;32m   1015\u001B[0m         \u001B[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001B[39;00m\n\u001B[1;32m   1016\u001B[0m         \u001B[38;5;66;03m# worker failures.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/queues.py:113\u001B[0m, in \u001B[0;36mQueue.get\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m block:\n\u001B[1;32m    112\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m deadline \u001B[38;5;241m-\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic()\n\u001B[0;32m--> 113\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    114\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_poll():\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py:262\u001B[0m, in \u001B[0;36m_ConnectionBase.poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    260\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_closed()\n\u001B[1;32m    261\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_readable()\n\u001B[0;32m--> 262\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py:429\u001B[0m, in \u001B[0;36mConnection._poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    428\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_poll\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout):\n\u001B[0;32m--> 429\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    430\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(r)\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py:936\u001B[0m, in \u001B[0;36mwait\u001B[0;34m(object_list, timeout)\u001B[0m\n\u001B[1;32m    933\u001B[0m     deadline \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic() \u001B[38;5;241m+\u001B[39m timeout\n\u001B[1;32m    935\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 936\u001B[0m     ready \u001B[38;5;241m=\u001B[39m \u001B[43mselector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    937\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ready:\n\u001B[1;32m    938\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [key\u001B[38;5;241m.\u001B[39mfileobj \u001B[38;5;28;01mfor\u001B[39;00m (key, events) \u001B[38;5;129;01min\u001B[39;00m ready]\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/selectors.py:416\u001B[0m, in \u001B[0;36m_PollLikeSelector.select\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    414\u001B[0m ready \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 416\u001B[0m     fd_event_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_selector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpoll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mInterruptedError\u001B[39;00m:\n\u001B[1;32m    418\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ready\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(11457, 6)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = loadmat(\"trainDataQuad.mat\")\n",
    "Xtrain = data['X']\n",
    "Ytrain = data['y']\n",
    "Xtrain.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}